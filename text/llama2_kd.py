# -*- coding: utf-8 -*-
"""llama2_kd.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oLhigVSgtxPqZJ3t8XnJ24M9a8KTB-Jb
"""

!pip install datasets evaluate bert-score sacrebleu --quiet
!huggingface-cli login
# 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

import time
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset
from bert_score import score  # For evaluation

# Distillation loss function
def distillation_loss(student_logits, teacher_logits, labels, temperature=2.0, alpha=0.5):
    """
    Compute the distillation loss: a weighted sum of KL divergence and standard cross-entropy.
    """
    # Scale logits by temperature
    teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)
    student_probs = F.log_softmax(student_logits / temperature, dim=-1)

    # KL divergence loss
    kl_loss = F.kl_div(student_probs, teacher_probs, reduction="batchmean") * (temperature**2)

    # Cross-entropy loss with true labels
    ce_loss = F.cross_entropy(student_logits.view(-1, student_logits.size(-1)), labels.view(-1))

    # Combine losses
    return alpha * kl_loss + (1 - alpha) * ce_loss

# Training function for the student model
def train_student_model(
    teacher_model, student_model, teacher_tokenizer, student_tokenizer, dataset, num_epochs=3, batch_size=4
):

    # Take 1/100th of the dataset
    subset_size = len(dataset) // 100
    subset_indices = list(range(subset_size))  # First 1/100th of the dataset
    dataset_subset = Subset(dataset, subset_indices)

    # DataLoader
    dataloader = DataLoader(dataset_subset, batch_size=batch_size, shuffle=True)

    # Optimizer
    optimizer = torch.optim.AdamW(student_model.parameters(), lr=5e-5)

    teacher_model.eval()  # Teacher remains fixed during distillation
    student_model.train()  # Student is trained

    for epoch in range(num_epochs):
        total_loss = 0.0
        for batch_idx, batch in enumerate(dataloader):
            # Tokenize input
            inputs = teacher_tokenizer(batch["text"], return_tensors="pt", truncation=True, padding=True).to(device)
            labels = inputs["input_ids"]  # Use input tokens as labels for language modeling

            # Get logits from teacher model
            with torch.no_grad():
                teacher_outputs = teacher_model(**inputs)
                teacher_logits = teacher_outputs.logits

            # Get logits from student model
            student_outputs = student_model(**inputs)
            student_logits = student_outputs.logits

            # Compute distillation loss
            loss = distillation_loss(student_logits, teacher_logits, labels)
            total_loss += loss.item()

            # Backpropagation
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # Print batch loss
            if batch_idx%10==0:
              print(f"Epoch {epoch + 1}, Batch {batch_idx + 1}/{len(dataloader)}: Loss = {loss.item():.4f}")

            torch.cuda.empty_cache()

        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch + 1}/{num_epochs} completed. Average Loss: {avg_loss:.4f}")


# Evaluation function (using BERTScore)
def evaluate_BERTScore(model, tokenizer, dataset, num_samples=10, prompt_length=400, gen_length=400):
    predictions = []
    references = []
    total_inference_time = 0.0
    memory_usages = []  # To store memory usage for each sample

    for i in range(num_samples):

        if device == "cuda":
            torch.cuda.reset_peak_memory_stats(device)
            initial_memory = torch.cuda.memory_allocated(device) / 1e6  # MB

        text = dataset[i]["text"]
        if len(text) < prompt_length + gen_length:
            continue

        # Split text into prompt and reference
        prompt = text[:prompt_length]
        reference = text[prompt_length : prompt_length + gen_length]

        # Tokenize input
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)

        # Measure inference time and memory usage
        torch.cuda.reset_peak_memory_stats(device)  # Reset memory stats
        start_time = time.time()
        with torch.no_grad():
            outputs = model.generate(**inputs, max_length=gen_length)
        inference_time = time.time() - start_time
        total_inference_time += inference_time

        # Decode prediction
        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)

        if device == "cuda":
            peak_memory = torch.cuda.max_memory_allocated(device) / 1e6  # MB
            memory_usage = peak_memory - initial_memory
        # Append to lists
        predictions.append(prediction)
        references.append(reference)
        if device == "cuda":
            memory_usages.append(memory_usage)



    # Compute BERTScore
    P, R, F1 = score(predictions, references, lang="en", verbose=True)

    avg_inference_time = total_inference_time / num_samples
    avg_memory_usage = sum(memory_usages) / len(memory_usages) if memory_usages else 0.0

    return {
        "precision": P.mean().item(),
        "recall": R.mean().item(),
        "f1": F1.mean().item(),
        "avg_inference_time": avg_inference_time,
        "avg_memory_usage_mb": avg_memory_usage,
        "max_memory_usage_mb": max(memory_usages) if memory_usages else 0.0,
    }

# Device setup
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load teacher and student models
teacher_model_name = "meta-llama/Llama-2-7b-hf"

teacher_model = AutoModelForCausalLM.from_pretrained(teacher_model_name).to(device)
teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)
if teacher_tokenizer.pad_token is None:
    teacher_tokenizer.pad_token = teacher_tokenizer.eos_token

# Load dataset
dataset = load_dataset("wikitext", "wikitext-103-v1")
train_dataset = dataset["train"]

student_model_name = "distilgpt2"  # Smaller model for distillation
student_model = AutoModelForCausalLM.from_pretrained(student_model_name).to(device)

# Resize student vocabulary to match teacher's
teacher_vocab_size = teacher_tokenizer.vocab_size
student_model.resize_token_embeddings(teacher_vocab_size)
student_tokenizer = teacher_tokenizer  # Use teacher tokenizer for inputs

# Train the student model
print("Starting training of the student model...")
train_student_model(teacher_model, student_model, teacher_tokenizer, student_tokenizer, train_dataset, num_epochs=1)

student_model = AutoModelForCausalLM.from_pretrained("\\").to(device)
student_tokenizer = teacher_tokenizer

# Evaluate the student model
print("Evaluating the teacher model...")
test_dataset = dataset["test"]
results = evaluate_BERTScore(teacher_model, teacher_tokenizer, test_dataset, num_samples=100)

print("\nEvaluation Results:")
print(f"Precision Score (BERTScore): {results['precision']:.4f}")
print(f"Recall Score (BERTScore): {results['recall']:.4f}")
print(f"F1 Score (BERTScore): {results['f1']:.4f}")
print(f"Average Inference Time: {results['avg_inference_time']:.4f}s")
print(f"Average Memory Usage: {results['avg_memory_usage_mb']:.4f}MB")
print(f"Max Memory Usage: {results['max_memory_usage_mb']:.4f}MB")

print("Evaluating the student model...")
test_dataset = dataset["test"]
results = evaluate_BERTScore(student_model, student_tokenizer, test_dataset, num_samples=100)

print("\nEvaluation Results:")
print(f"Precision Score (BERTScore): {results['precision']:.4f}")
print(f"Recall Score (BERTScore): {results['recall']:.4f}")
print(f"F1 Score (BERTScore): {results['f1']:.4f}")
print(f"Average Inference Time: {results['avg_inference_time']:.4f}s")
print(f"Average Memory Usage: {results['avg_memory_usage_mb']:.4f}MB")
print(f"Max Memory Usage: {results['max_memory_usage_mb']:.4f}MB")